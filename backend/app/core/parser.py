# import os 
# import json
# import re
# from collections import defaultdict

# def parse_logs(log_dir=None):
#     access_counts = defaultdict(int)
#     access_times = defaultdict(list)
#     total_good, total_bad = 0, 0

#     if not log_dir:
#         log_dir = os.getenv("LOG_DIR", "/mnt/nfs-logs")

#     if not os.path.exists(log_dir):
#         print(f"âŒ Log directory does not exist: {log_dir}")
#         return {}, {}

#     cwd_cache = {}

#     for filename in sorted(os.listdir(log_dir)):
#         if not filename.endswith(".ndjson"):
#             continue

#         path = os.path.join(log_dir, filename)
#         print(f"ðŸ“„ Processing file: {path}")
#         good, bad = 0, 0

#         with open(path, "r", encoding="utf-8", errors="ignore") as f:
#             for line in f:
#                 try:
#                     if "type=CWD" in line and 'cwd="' in line:
#                         match = re.search(r'cwd="([^"]+)"', line)
#                         if match:
#                             event_id = extract_event_id(line)
#                             cwd_cache[event_id] = match.group(1)

#                     if "type=PATH" in line and 'name=' in line:
#                         event_id = extract_event_id(line)
#                         cwd = cwd_cache.get(event_id, "")
#                         name_match = re.search(r'name="([^"]+)"', line)
#                         if not name_match:
#                             continue
#                         name = name_match.group(1)

#                         full_path = os.path.join(cwd, name)
#                         if full_path.startswith("/mnt/data"):
#                             access_counts[full_path] += 1
#                             access_times[full_path].append("")
#                             good += 1
#                     else:
#                         bad += 1

#                 except Exception as e:
#                     print(f"âŒ Error: {e}")
#                     bad += 1

#         total_good += good
#         total_bad += bad
#         print(f"âœ… Parsed {good} good entries, âš ï¸ Skipped {bad} bad entries.")

#     print(f"\nðŸ“Š Found {len(access_counts)} unique paths. Total good: {total_good}, bad: {total_bad}")
#     return access_counts, access_times

# def extract_event_id(line):
#     match = re.search(r'audit\(\d+\.\d+:(\d+)\)', line)
#     return match.group(1) if match else None

import os 
import json
import re
from collections import defaultdict

def parse_logs(log_dir=None):
    access_counts = defaultdict(int)
    access_times = defaultdict(list)
    total_good, total_bad = 0, 0

    if not log_dir:
        log_dir = os.getenv("LOG_DIR", "/mnt/nfs-logs")

    if not os.path.exists(log_dir):
        print(f"âŒ Log directory does not exist: {log_dir}")
        return {}, {}

    cwd_cache = {}

    for filename in sorted(os.listdir(log_dir)):
        if not filename.endswith(".ndjson"):
            continue

        path = os.path.join(log_dir, filename)
        print(f"ðŸ“„ Processing file: {path}")
        good, bad = 0, 0

        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                try:
                    if "type=CWD" in line and 'cwd="' in line:
                        match = re.search(r'cwd="([^"]+)"', line)
                        if match:
                            event_id = extract_event_id(line)
                            cwd_cache[event_id] = match.group(1)

                    if "type=PATH" in line and 'name=' in line:
                        event_id = extract_event_id(line)
                        cwd = cwd_cache.get(event_id, "")
                        name_match = re.search(r'name="([^"]+)"', line)
                        if not name_match:
                            continue
                        name = name_match.group(1)

                        full_path = os.path.join(cwd, name)
                        if full_path.startswith("/mnt/data"):
                            access_counts[full_path] += 1
                            access_times[full_path].append("")
                            good += 1
                    else:
                        bad += 1

                except Exception as e:
                    print(f"âŒ Error: {e}")
                    bad += 1

        total_good += good
        total_bad += bad
        print(f"âœ… Parsed {good} good entries, âš ï¸ Skipped {bad} bad entries.")

    print(f"\nðŸ“Š Found {len(access_counts)} unique paths. Total good: {total_good}, bad: {total_bad}")
    return access_counts, access_times

def extract_event_id(line):
    match = re.search(r'audit\(\d+\.\d+:(\d+)\)', line)
    return match.group(1) if match else None

# Absolute rule-based tiering logic
def classify_tier_by_threshold(count: int) -> str:
    if count >= 100:
        return "HOT"
    elif 20 <= count < 100:
        return "WARM"
    else:
        return "COLD"

def apply_rule_based_tiering(access_counts: dict) -> dict:
    tiered = {}
    for path, count in access_counts.items():
        tiered[path] = classify_tier_by_threshold(count)
    return tiered
